{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5de5060b-7b01-41a0-9140-eee106d706ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "## langchain base:\n",
    "from langchain.schema import Document\n",
    "## ollama \n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "## chat history stuff\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, RemoveMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "## lang graph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "## visualizing lang graph\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "\n",
    "## document loaders & text splitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders.parsers import BS4HTMLParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, html, HTMLSectionSplitter\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_community.embeddings import FastEmbedEmbeddings\n",
    "\n",
    "\n",
    "## requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## chroma vector store\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "\n",
    "## others\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "import os\n",
    "\n",
    "from GP_Copilot.copilot import LLM_langchain\n",
    "\n",
    "\n",
    "## memory stuff and langgraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import Annotated\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102d32ce-5e02-4b38-9c1c-d74b4a0a3f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a56313d6ba4ccf9e710a6ddb928d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d687dfcf9ebb45b083e8b56733a0125e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bbe6e8158e4ecf8830314c87b1ee83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5310acde79454444b56777cafa5b7e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038c7575709842b595965363068084db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ea2b65437a4b0e849c51fe018db3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_optimized.onnx:   0%|          | 0.00/66.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever found. Number of documents in collection: 1073\n"
     ]
    }
   ],
   "source": [
    "vec_path = 'GP_copilot/chroma'\n",
    "vector_store = LLM_langchain.get_retriever(path = vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3365a4fd-00a3-4433-a401-81f7711af3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a80c51013084871a7158ce12cfb81a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever found. Number of documents in collection: 1073\n"
     ]
    }
   ],
   "source": [
    "chain = LLM_langchain.get_chain(retriever_path = vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010a1d2c-af9a-4533-b771-c79d1f7e0112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It seems like you\\'re just saying \"Hello?\" without asking a specific question.\\n\\nHowever, I can try to help you with something related to GenePattern or Genomics if you\\'d like! \\n\\nIf not, feel free to ask your actual question, and I\\'ll do my best to assist you.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('Hello?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f300d5-2c11-4f7e-aacd-af65d168621e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35d678e8-8965-4f94-92bc-2b0621883888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    model_type : str\n",
    "\n",
    "initial_state = {\n",
    "    'messages' : [], \n",
    "    'model_type' : 'llama'\n",
    "}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# llm_with_tools = LLM_langchain.get_model('llama', False)\n",
    "llm_with_tools = chain\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [chain.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "861c68d7-824f-48f1-9a34-f32bba41e29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hello i am edwin', additional_kwargs={}, response_metadata={}, id='6f32e3d1-7ce3-4f13-91ef-1f6feee76b8b'),\n",
       "  HumanMessage(content=\"It seems like you're trying to start a conversation, but I don't see any specific question about running a workflow or bioinformatics-related query. Could you please provide more context or ask a specific question? I'll be happy to help with your inquiry!\", additional_kwargs={}, response_metadata={}, id='9271ad34-296d-475e-9a9d-ffb92ffadb4e'),\n",
       "  HumanMessage(content='hello i am edwin', additional_kwargs={}, response_metadata={}, id='9ccb876f-a18c-43d1-b73b-e4da7c3c1b7d'),\n",
       "  HumanMessage(content='It appears that you have a list of objects, some of which are `Document` objects and others are `HumanMessage` objects. The question is asking to extract the content from the `HumanMessage` object.\\n\\nHere\\'s how you can do it:\\n\\n```python\\n# Assuming this is your data structure\\ndata = [\\n    Document(metadata={\\'category\\': \\'module\\', \\'description\\': \\'Extract specified parameters (dimensions) from an FCS data file.\\', \\'task_name\\': \\'ExtractFCSParameters\\'}, page_content=\\'\\\\n        Module name is ExtractFCSParameters,\\\\n        description is Extract specified parameters (dimensions) from an FCS data file., \\\\n        link to module is: https://cloud.genepattern.org//gp/pages/index.jsf?lsid=urn:lsid:broad.mit.edu:cancer.software.genepattern.module.analysis:00180:2\\\\n    \\'),\\n    HumanMessage(content=\\'hello i am edwin\\', additional_kwargs={}, response_metadata={}, id=\\'6f32e3d1-7ce3-4f13-91ef-1f6feee76b8b\\'),\\n    # ... rest of your data\\n]\\n\\n# Extract the content from all HumanMessage objects\\nhuman_message_contents = [message.content for message in data if isinstance(message, HumanMessage)]\\n\\nprint(human_message_contents)\\n```\\n\\nThis will output:\\n\\n```python\\n[\\'hello i am edwin\\', \"It seems like you\\'re trying to start a conversation, but I don\\'t see any specific question about running a workflow or bioinformatics-related query. Could you please provide more context or ask a specific question? I\\'ll be happy to help with your inquiry!\"]\\n```\\n\\nNote that this assumes that the `HumanMessage` objects are in the same list as the rest of your data, and that they have a `content` attribute. If your actual data structure is different, you may need to modify the code accordingly.', additional_kwargs={}, response_metadata={}, id='fdf259ff-36bc-4bbd-a7a5-69aa47f4fafb'),\n",
       "  HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='3a67604b-578b-41ac-9817-670f04eed25a'),\n",
       "  HumanMessage(content='You want to extract the content from all `HumanMessage` objects in a given list. \\n\\nHere is how you can do it:\\n\\n```python\\ndef extract_human_message_contents(data):\\n    human_message_contents = [message.content for message in data if isinstance(message, HumanMessage)]\\n    return human_message_contents\\n\\ndata = [\\n    Document(metadata={\\'category\\': \\'module\\', \\'description\\': \\'Extract specified parameters (dimensions) from an FCS data file.\\', \\'task_name\\': \\'ExtractFCSParameters\\'}, page_content=\\'Module name is ExtractFCSParameters,\\\\n        description is Extract specified parameters (dimensions) from an FCS data file.,\\\\n        link to module is: https://cloud.genepattern.org//gp/pages/index.jsf?lsid=urn:lsid:broad.mit.edu:cancer.software.genepattern.module.analysis:00180:2\\\\n    \\'),\\n    HumanMessage(content=\\'hello i am edwin\\', additional_kwargs={}, response_metadata={}, id=\\'6f32e3d1-7ce3-4f13-91ef-1f6feee76b8b\\'),\\n    # ... rest of your data\\n]\\n\\nhuman_message_contents = extract_human_message_contents(data)\\nprint(human_message_contents)\\n```\\n\\nThis will output:\\n\\n```python\\n[\\'hello i am edwin\\', \"It seems like you\\'re trying to start a conversation, but I don\\'t see any specific question about running a workflow or bioinformatics-related query. Could you please provide more context or ask a specific question? I\\'ll be happy to help with your inquiry!\"]\\n```\\n\\nNote that this assumes that the `HumanMessage` objects are in the same list as the rest of your data, and that they have a `content` attribute. If your actual data structure is different, you may need to modify the code accordingly.', additional_kwargs={}, response_metadata={}, id='5d0ea0ef-6e1b-4cf6-9b17-40eae43b9745'),\n",
       "  HumanMessage(content='tell me about seurat', additional_kwargs={}, response_metadata={}, id='d639005a-a752-47c3-a3af-49e49cf1451a'),\n",
       "  HumanMessage(content='You want to extract the content from all `HumanMessage` objects in a given list. Here\\'s how you can do it:\\n\\n```python\\n# Assuming this is your data structure\\ndata = [\\n    Document(metadata={\\'category\\': \\'module\\', \\'description\\': \\'Extract specified parameters (dimensions) from an FCS data file.\\', \\'task_name\\': \\'ExtractFCSParameters\\'}, \\n             page_content=\\'Module name is ExtractFCSParameters,\\\\n        description is Extract specified parameters (dimensions) from an FCS data file., \\\\n        link to module is: https://cloud.genepattern.org//gp/pages/index.jsf?lsid=urn:lsid:broad.mit.edu:cancer.software.genepattern.module.analysis:00180:2\\\\n    \\'),\\n    HumanMessage(content=\\'hello i am edwin\\', additional_kwargs={}, response_metadata={}, id=\\'6f32e3d1-7ce3-4f13-91ef-1f6feee76b8b\\'),\\n    # ... rest of your data\\n]\\n\\n# Extract the content from all HumanMessage objects\\nhuman_message_contents = [message.content for message in data if isinstance(message, HumanMessage)]\\n\\nprint(human_message_contents)\\n```\\n\\nThis will output:\\n\\n```python\\n[\\'hello i am edwin\\', \"It seems like you\\'re trying to start a conversation, but I don\\'t see any specific question about running a workflow or bioinformatics-related query. Could you please provide more context or ask a specific question? I\\'ll be happy to help with your inquiry!\"]\\n```\\n\\nNote that this assumes that the `HumanMessage` objects are in the same list as the rest of your data, and that they have a `content` attribute. If your actual data structure is different, you may need to modify the code accordingly.', additional_kwargs={}, response_metadata={}, id='b6406d19-8b9b-42a4-ac0e-f4eee9155b60'),\n",
       "  HumanMessage(content='what did I ask you before?', additional_kwargs={}, response_metadata={}, id='6aa215c7-c345-4ed6-9a48-839aef79cbbf'),\n",
       "  HumanMessage(content='The code provided is a list of `HumanMessage` objects, each with a unique ID and content. The task is to extract the content from all `HumanMessage` objects in this list.\\n\\nHere\\'s how you can do it:\\n\\n```python\\ndef extract_human_message_contents(data):\\n    human_message_contents = [message.content for message in data if isinstance(message, HumanMessage)]\\n    return human_message_contents\\n\\ndata = [\\n    Document(metadata={\\'category\\': \\'module\\', \\'description\\': \\'Extract specified parameters (dimensions) from an FCS data file.\\', \\'task_name\\': \\'ExtractFCSParameters\\'}, \\n             page_content=\\'Module name is ExtractFCSParameters,\\\\n        description is Extract specified parameters (dimensions) from an FCS data file.,\\\\n        link to module is: https://cloud.genepattern.org//gp/pages/index.jsf?lsid=urn:lsid:broad.mit.edu:cancer.software.genepattern.module.analysis:00180:2\\\\n    \\'),\\n    HumanMessage(content=\\'hello i am edwin\\', additional_kwargs={}, response_metadata={}, id=\\'6f32e3d1-7ce3-4f13-91ef-1f6feee76b8b\\'),\\n    # ... rest of your data\\n]\\n\\nhuman_message_contents = extract_human_message_contents(data)\\nprint(human_message_contents)\\n```\\n\\nThis will output:\\n\\n```python\\n[\\'hello i am edwin\\', \"It seems like you\\'re trying to start a conversation, but I don\\'t see any specific question about running a workflow or bioinformatics-related query. Could you please provide more context or ask a specific question? I\\'ll be happy to help with your inquiry!\"]\\n```\\n\\nNote that this assumes that the `HumanMessage` objects are in the same list as the rest of your data, and that they have a `content` attribute. If your actual data structure is different, you may need to modify the code accordingly.', additional_kwargs={}, response_metadata={}, id='8c43e699-22fa-4e5f-8456-68b0e983ef85'),\n",
       "  HumanMessage(content='tell me about seurat', additional_kwargs={}, response_metadata={}, id='ffc9faf7-c520-45e0-967c-94d4c605d427'),\n",
       "  AIMessage(content=\"It seems like we had a conversation earlier, but I don't see any specific question about SEURAT. Could you please provide more context or ask a specific question about SEURAT? I'll be happy to help with your inquiry!\\n\\nHowever, if you're interested in learning more about SEURAT, it's a popular R package for single-cell RNA sequencing data analysis. It provides a comprehensive framework for processing and analyzing scRNA-seq data, including data normalization, dimensionality reduction, clustering, and visualization.\\n\\nIf you have any specific questions or topics related to SEURAT that you'd like to discuss, I'm here to help!\", additional_kwargs={}, response_metadata={'model': 'llama3.2-vision', 'created_at': '2024-12-13T00:07:28.062632Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 20978266791, 'load_duration': 4638752625, 'prompt_eval_count': 1629, 'prompt_eval_duration': 9616000000, 'eval_count': 131, 'eval_duration': 6343000000}, id='run-ca9b7a35-92d3-42bd-96fd-8c7919eaa8eb-0', usage_metadata={'input_tokens': 1629, 'output_tokens': 131, 'total_tokens': 1760}),\n",
       "  HumanMessage(content='what did I ask you before?', additional_kwargs={}, response_metadata={}, id='f6bcdb0f-1ea2-4859-a963-c0f24d058579'),\n",
       "  AIMessage(content='You asked me \"tell me about seurat\" earlier. However, it seems that the conversation started with your introduction: \"hello i am edwin\". Then, there was a code snippet and some discussion about extracting content from `HumanMessage` objects in a list.\\n\\nBefore that, you also asked \"what\\'s my name?\" to which I responded with \"Edwin\"!', additional_kwargs={}, response_metadata={'model': 'llama3.2-vision', 'created_at': '2024-12-13T00:07:32.160471Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 4003509583, 'load_duration': 41937208, 'prompt_eval_count': 1776, 'prompt_eval_duration': 229000000, 'eval_count': 77, 'eval_duration': 3712000000}, id='run-c9ad5349-e897-4b62-9133-032edf7d1271-0', usage_metadata={'input_tokens': 1776, 'output_tokens': 77, 'total_tokens': 1853}),\n",
       "  HumanMessage(content='what did I ask you before?', additional_kwargs={}, response_metadata={}, id='4f38cec7-da8e-42b9-b9b9-da484a718434'),\n",
       "  HumanMessage(content='The code provided is a Python script that extracts the content of `HumanMessage` objects from a list. The script defines a function `extract_human_message_contents` that takes a list as input and returns a list of contents of `HumanMessage` objects.\\n\\nHere\\'s a breakdown of the code:\\n\\n1. The function `extract_human_message_contents` is defined with a docstring explaining its purpose.\\n2. The function takes a list as input, which is expected to contain `HumanMessage` objects.\\n3. The function uses a list comprehension to extract the `content` attribute from each `HumanMessage` object in the input list.\\n4. The extracted contents are returned as a new list.\\n\\nThe code also includes some example usage of the function, where it is called with a sample list containing several `HumanMessage` objects and their corresponding contents are printed to the console.\\n\\nTo write this code based on the provided specification, you can simply copy and paste the code into your Python environment. If you want to create the code from scratch, here\\'s a step-by-step guide:\\n\\n1. Define a function named `extract_human_message_contents`.\\n2. Add a docstring explaining the purpose of the function.\\n3. Define the function signature with a single argument, which is expected to be a list.\\n4. Use a list comprehension to extract the `content` attribute from each `HumanMessage` object in the input list.\\n5. Return the extracted contents as a new list.\\n\\nHere\\'s the code again for reference:\\n\\n```python\\ndef extract_human_message_contents(lst):\\n    \"\"\"\\n    Extracts the content of HumanMessage objects from a list.\\n\\n    Args:\\n        lst (list): A list containing HumanMessage objects.\\n\\n    Returns:\\n        list: A list of contents of HumanMessage objects.\\n    \"\"\"\\n    return [msg.content for msg in lst if isinstance(msg, HumanMessage)]\\n\\n# Example usage\\nhuman_messages = [\\n    HumanMessage(content=\"hello i am edwin\"),\\n    AIMessage(content=\"It seems like we had a conversation earlier...\"),\\n    # ...\\n]\\n\\ncontents = extract_human_message_contents(human_messages)\\nprint(contents)\\n```\\n\\nNote that this code assumes the existence of `HumanMessage` and `AIMessage` classes, which are not defined in the provided specification. You may need to modify the code to match your specific use case.', additional_kwargs={}, response_metadata={}, id='8f2aaeb4-08b4-46c3-886c-35eda99fe196'),\n",
       "  HumanMessage(content='tell me about seurat', additional_kwargs={}, response_metadata={}, id='94a2fad0-d067-48be-834c-30407d29be75'),\n",
       "  HumanMessage(content='The code you provided is a Python script that extracts the content of `HumanMessage` objects from a list. The script defines a function `extract_human_message_contents` that takes a list as input and returns a list of contents of `HumanMessage` objects.\\n\\nHere\\'s a breakdown of the code:\\n\\n1. The function `extract_human_message_contents` is defined with a docstring explaining its purpose.\\n2. The function takes a list as input, which is expected to contain `HumanMessage` objects.\\n3. The function uses a list comprehension to extract the `content` attribute from each `HumanMessage` object in the input list.\\n4. The extracted contents are returned as a new list.\\n\\nTo write this code based on the provided specification, you can simply copy and paste the code into your Python environment. If you want to create the code from scratch, here\\'s a step-by-step guide:\\n\\n1. Define a function named `extract_human_message_contents`.\\n2. Add a docstring explaining the purpose of the function.\\n3. Define the function signature with a single argument, which is expected to be a list.\\n4. Use a list comprehension to extract the `content` attribute from each `HumanMessage` object in the input list.\\n5. Return the extracted contents as a new list.\\n\\nHere\\'s the code again for reference:\\n\\n```python\\ndef extract_human_message_contents(lst):\\n    \"\"\"Extracts the content of HumanMessage objects from a list.\\n\\n    Args:\\n        lst (list): A list containing HumanMessage objects.\\n\\n    Returns:\\n        list: A list of contents of HumanMessage objects.\\n    \"\"\"\\n    return [msg.content for msg in lst if isinstance(msg, HumanMessage)]\\n\\n# Example usage\\nhuman_messages = [\\n    HumanMessage(content=\"hello i am edwin\"),\\n    AIMessage(content=\"It seems like we had a conversation earlier...\"),\\n    # ...\\n]\\n\\ncontents = extract_human_message_contents(human_messages)\\nprint(contents)\\n```\\n\\nNote that this code assumes the existence of `HumanMessage` and `AIMessage` classes, which are not defined in the provided specification. You may need to modify the code to match your specific use case.\\n\\nTo answer your question directly: The code extracts the content of all `HumanMessage` objects from a list and returns them as a new list.', additional_kwargs={}, response_metadata={}, id='3924d55b-aba0-445e-b24a-ca9b12da67d7')]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "user_input = \"tell me about seurat\"\n",
    "graph.invoke(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0918c3f-7133-428c-aea5-10964f22c024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd472319-9444-4ae7-8524-27b366e3bc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6fba1-2797-46bf-85d8-7491d3df0eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf3f07e-e65f-4424-ae38-186447f53c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9dec77-a482-46ae-9675-5d78da42a780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5dfc3-096b-48a4-b5e3-6f79939c0a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a111de0-33f3-4653-a622-3d251d6a993d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3e28097-973c-4a0d-b500-ad2ae94a6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the HTML file\n",
    "with open('/Users/edwinhuang/Downloads/genepattern module & links .html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all tables\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "# Find all rows with class \"task-title\"\n",
    "task_rows = soup.find_all(\"tr\", class_=\"task-title\")\n",
    "\n",
    "# List to store documents for the vector store\n",
    "documents = []\n",
    "\n",
    "# Extract task names, descriptions, and links\n",
    "for row in task_rows:\n",
    "    # Get the task name (inside the third <td>)\n",
    "    task_name = row.find_all(\"td\")[2].contents[0].strip()\n",
    "\n",
    "    # Get the description (inside <span class=\"smalltype5\">)\n",
    "    description_span = row.find(\"span\", class_=\"smalltype5\")\n",
    "    task_description = description_span.get_text(strip=True) if description_span else \"No description\"\n",
    "\n",
    "    # Find the next sibling <tr> and extract the link if present\n",
    "    next_row = row.find_next_sibling(\"tr\")\n",
    "    link = next_row.find(\"a\", href=True)['href'] if next_row and next_row.find(\"a\", href=True) else \"No link\"\n",
    "\n",
    "    # Create a document with the specified namespace\n",
    "\n",
    "    row = f'''\n",
    "        Module name is {task_name},\n",
    "        description is {task_description}, \n",
    "        link to module is: https://cloud.genepattern.org/{link}\n",
    "    '''\n",
    "    \n",
    "    document = Document(\n",
    "        page_content = row,\n",
    "        metadata={\n",
    "         \"task_name\": task_name,\n",
    "        \"description\": task_description,\n",
    "        \"category\": \"module\"   \n",
    "        })\n",
    "    \n",
    "    documents.append(document)\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "b = vector_store.add_documents(documents, ids = uuids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa70a13e-a0c8-4d67-b729-dd7aac175978",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "405eb5a6-9843-40be-82bc-31604376d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import (\n",
    "    ConfigurableField,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "    You are a bioinformatics wizard who works for the GenePattern team.\n",
    "    Your job is to answer bioinformatics related questions about running a workflow.\n",
    "    The user might provide information in different formats, such as text and images.\n",
    "    \n",
    "    Do not describe tools that are not in the vector store, instead respond\n",
    "    with \"That tool is not currently available in GenePattern. Feel free to contact\n",
    "    the GenePattern team if you think it would be a good addition to our repository. Email: edh021@cloud.ucsd.edu\"\n",
    "    Provide input file formats when giving instructions on how to run modules\n",
    "    or tools. Only give module suggestions for modules in GenePattern.\n",
    "    Do not tell users to “go to GenePattern and log in”.\n",
    "    Answer the following questions using all your knowledge\n",
    "    and providing as much detail as possible with step-by-step instructions.\n",
    "    \n",
    "    Use the following context to answer the question: {context}. \n",
    "    \n",
    "    The question is: {question}.\n",
    "    \n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", template),\n",
    "        (\n",
    "            \"user\",\n",
    "            [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": \"data:image/jpeg;base64,{image_data}\"},\n",
    "                }\n",
    "            ],\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "configurable_retriever = retriever.configurable_fields(\n",
    "    search_kwargs=ConfigurableField(\n",
    "        id=\"search_kwargs\",\n",
    "        name=\"Search Kwargs\",\n",
    "        description=\"The search kwargs to use\",\n",
    "    ) ## remember when loading docs we need to add in a namespace or soemthing\n",
    ")\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever = retriever, llm = model)\n",
    "chain = (\n",
    "    {\"context\": retriever_from_llm, \"question\": RunnablePassthrough(), 'image_data':RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd759306-d5d7-4292-9fa0-7f4690ef6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
    "with open(\"/Users/edwinhuang/Desktop/IMG_2198.JPG\", \"rb\") as image_file:\n",
    "    base64_encoded_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# pil_image = Image.open(\"/Users/edwinhuang/Desktop/IMG_2198.JPG\")\n",
    "# image_64 = convert_to_base64(pil_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "74c35049-4e23-4e32-944c-c8f4c9a94af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems I don't have any information about the image you're referring to. Could you please describe it or provide more context so I can better understand what you'd like me to do?\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField, RunnablePassthrough\n",
    "\n",
    "# Open the image file and read it as bytes\n",
    "with open(\"/Users/edwinhuang/Desktop/IMG_2198.JPG\", \"rb\") as image_file:\n",
    "    image_data = BytesIO(image_file.read())\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "    You are a bioinformatics wizard who works for the GenePattern team.\n",
    "    Your job is to answer bioinformatics related questions about running a workflow.\n",
    "    The user might provide information in different formats, such as text and images.\n",
    "\n",
    "    Do not describe tools that are not in the vector store, instead respond\n",
    "    with \"That tool is not currently available in GenePattern. Feel free to contact\n",
    "    the GenePattern team if you think it would be a good addition to our repository. Email: edh021@cloud.ucsd.edu\"\n",
    "    Provide input file formats when giving instructions on how to run modules\n",
    "    or tools. Only give module suggestions for modules in GenePattern.\n",
    "    Do not tell users to “go to GenePattern and log in”.\n",
    "    Answer the following questions using all your knowledge\n",
    "    and providing as much detail as possible with step-by-step instructions.\n",
    "\n",
    "    Use the following context to answer the question: {context}. \n",
    "\n",
    "    The question is: {question}.\n",
    "\"\"\"\n",
    "\n",
    "template = \"Describe the image\"\n",
    "\n",
    "# Define the prompt using ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"user\", \"The user has provided an image.\")\n",
    "])\n",
    "\n",
    "# Create a HumanMessage object with the image data\n",
    "message = HumanMessage(\n",
    "    content=\"Describe the image\",\n",
    "    images=[image_data]  # Pass the image as a list of file-like objects\n",
    ")\n",
    "\n",
    "# Define the retriever and chain\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=retriever, llm=model)\n",
    "chain = (\n",
    "    {\"context\": retriever_from_llm, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the chain with the HumanMessage\n",
    "response = chain.invoke(message)\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db46710f-3dd3-479b-b179-c70227dbf58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='Describe the image', additional_kwargs={}, response_metadata={}, images=[<_io.BytesIO object at 0x157950a90>])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4bb47100-903a-4f25-9a5c-7c4b1ea2a543",
   "metadata": {},
   "outputs": [
    {
     "ename": "RequestError",
     "evalue": "image must be bytes, path-like object, or file-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m message \u001b[38;5;241m=\u001b[39m HumanMessage(\n\u001b[1;32m     10\u001b[0m     content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescribe the image\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     images\u001b[38;5;241m=\u001b[39m[image_data]  \u001b[38;5;66;03m# Pass the image as a list of file-like objects\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Invoke the chain using the HumanMessage\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_ollama/chat_models.py:644\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    639\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 644\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    648\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    649\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    650\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    655\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_ollama/chat_models.py:545\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    544\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mChatGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAIMessageChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_ollama/chat_models.py:527\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    518\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    519\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         tools\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mollama_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ollama/_client.py:234\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages \u001b[38;5;129;01mor\u001b[39;00m []:\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 234\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m_encode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_stream(\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    238\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m   stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    249\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ollama/_client.py:234\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages \u001b[38;5;129;01mor\u001b[39;00m []:\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 234\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43m_encode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_stream(\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    238\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m   stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    249\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ollama/_client.py:938\u001b[0m, in \u001b[0;36m_encode_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m b \u001b[38;5;241m:=\u001b[39m _as_bytesio(image):\n\u001b[1;32m    936\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m b64encode(b\u001b[38;5;241m.\u001b[39mread())\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 938\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage must be bytes, path-like object, or file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRequestError\u001b[0m: image must be bytes, path-like object, or file-like object"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Open the image file and read it as bytes\n",
    "with open(\"/Users/edwinhuang/Desktop/IMG_2198.JPG\", \"rb\") as image_file:\n",
    "    image_data = BytesIO(image_file.read())\n",
    "\n",
    "# Create a HumanMessage object with image data\n",
    "message = HumanMessage(\n",
    "    content=\"Describe the image\",\n",
    "    images=[image_data]  # Pass the image as a list of file-like objects\n",
    ")\n",
    "\n",
    "# Invoke the chain using the HumanMessage\n",
    "response = chain.invoke(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eecc0088-e8ef-4b2a-9375-60a3342a5895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2ab3a9f-c56d-42b3-8595-b751bb03a727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you've provided a large block of text that appears to be a hexadecimal string. However, I'm not sure what you're trying to accomplish or what kind of output you're expecting.\n",
      "\n",
      "If you could provide more context or clarify your question, I'd be happy to try and assist you. Are you trying to:\n",
      "\n",
      "* Convert the hexadecimal string back into its original text form?\n",
      "* Extract specific information from the string?\n",
      "* Use the string for a specific purpose (e.g., encryption, decryption)?\n",
      "\n",
      "Please let me know, and I'll do my best to help!\n"
     ]
    }
   ],
   "source": [
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"describe the image\"},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_encoded_data}\"},\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(chain.invoke([\n",
    "        {\"type\": \"text\", \"text\": \"describe the image\"},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_encoded_data}\"},\n",
    "        },\n",
    "    ],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdae4272-427a-4c82-bbe1-645faf04fd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're interested in running Seurat on GenePattern!\n",
      "\n",
      "Yes, you can definitely run Seurat on GenePattern. We have several modules related to Seurat that you can use.\n",
      "\n",
      "One of the most relevant modules is `Seurat.IntegrateData`, which performs batch correction for Seurat objects. You can also consider using `Seurat.Preprocessing` for Seurat preprocessing tasks.\n",
      "\n",
      "Additionally, we have a companion module called `seuratclustertestedw` (or simply `Seurat.Clustering`) that performs clustering and marker identification on single cell data.\n",
      "\n",
      "To get started with running Seurat on GenePattern, you'll need to upload your Seurat object file (.RData or .rds) as input to the corresponding module. For example, if you want to use `Seurat.IntegrateData`, you can upload your Seurat object file as a \"Seurat Object\" input.\n",
      "\n",
      "Here are some step-by-step instructions:\n",
      "\n",
      "1. Go to the GenePattern interface and navigate to the \"Modules\" tab.\n",
      "2. Search for \"Seurat\" in the module search bar, and select one of the relevant modules (e.g., `Seurat.IntegrateData`, `Seurat.Preprocessing`, or `seuratclustertestedw`).\n",
      "3. Click on the selected module to open its interface.\n",
      "4. Upload your Seurat object file (.RData or .rds) as input to the corresponding field (e.g., \"Seurat Object\" for `Seurat.IntegrateData`).\n",
      "5. Configure any additional parameters required by the module, such as batch correction settings for `Seurat.IntegrateData`.\n",
      "6. Click on the \"Run\" button to execute the module and generate output.\n",
      "\n",
      "If you have any specific questions or need further assistance with running Seurat on GenePattern, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"can i run seurat on genepattern?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53f010be-be3d-4e52-b1f8-299ce5101be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that calls the model\n",
    "## langgraph\n",
    "demo_ephemeral_chat_history = [\n",
    "    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
    "    AIMessage(content=\"Hello!\"),\n",
    "    HumanMessage(content=\"How are you today?\"),\n",
    "    AIMessage(content=\"Fine thanks!\"),\n",
    "]\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"Answer all questions to the best of your ability.\"\n",
    "        \"The provided chat history includes a summary of the earlier conversation.\"\n",
    "\n",
    "    )\n",
    "    system_message = SystemMessage(content=system_prompt)\n",
    "    message_history = state[\"messages\"][:-1]  # exclude the most recent user input\n",
    "    # Summarize the messages if the chat history reaches a certain size\n",
    "    if len(message_history) >= 4:\n",
    "        last_human_message = state[\"messages\"][-1]\n",
    "        # Invoke the model to generate conversation summary\n",
    "        summary_prompt = (\n",
    "            \"Distill the above chat messages into a single summary message. \"\n",
    "            \"Include as many specific details as you can.\"\n",
    "        )\n",
    "        summary_message = model.invoke(\n",
    "            message_history + [HumanMessage(content=summary_prompt)]\n",
    "        )\n",
    "\n",
    "        # Delete messages that we no longer want to show up\n",
    "        delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"]]\n",
    "        # Re-add user message\n",
    "        human_message = HumanMessage(content=last_human_message.content)\n",
    "        # Call the model with summary & response\n",
    "        response = model.invoke([system_message, summary_message, human_message])\n",
    "        message_updates = [summary_message, human_message, response] + delete_messages\n",
    "    else:\n",
    "        message_updates = model.invoke([system_message] + state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": message_updates}\n",
    "\n",
    "\n",
    "# Define the node and edge\n",
    "\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ecf75e54-89e3-4e5a-b9c1-9321ec67d390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1090c594-107d-4378-b210-db0c0fc727dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "resp = app.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history\n",
    "        + [HumanMessage(content=\"What is my name?\")]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"3\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6f0b84a-f5c8-4d2e-a026-89d75d74143f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Here\\'s a summary of our conversation:\\n\\nNemo (a human) interacted with an AI, exchanging greetings and asking about its status (\"Hello!\", \"How are you today?\", \"What is my name?\"). The AI responded with neutral phrases (\"Fine thanks!\" and \"Hello!\"), and Nemo confirmed the correct response to the question about his own name.', additional_kwargs={}, response_metadata={}, id='d23c3189-46e8-4487-a24d-4256a14b256c'),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='8f36c4df-c504-4604-968c-0fa987b4beb5'),\n",
       "  HumanMessage(content='You mentioned earlier that I correctly identified your name as \"Nemo\". Is there anything else you\\'d like to discuss or ask?', additional_kwargs={}, response_metadata={}, id='ed70e812-dfd0-4e41-ae01-4775e39bacc3')]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367b854-cc7d-42cb-85c8-17352782821c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee35be20-2389-4056-a724-0a54b52c3de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Here is a summary of the conversation:\\n\\nA human (Nemo) interacted with an AI, exchanging greetings and asking about its status. The AI responded with neutral phrases (\"Fine thanks!\" and \"Hello!\"). Nemo then asked the AI what his name was, and the AI confirmed that Nemo had correctly identified it earlier. After a loop of questioning and answering, the human pointed out the circular nature of their conversation, where Nemo initially asked about its own name and later confirmed it, followed by asking again and repeating the process. The human then offered to continue chatting on almost any topic, suggesting conversation starters if needed.', additional_kwargs={}, response_metadata={}, id='9cc68477-98c1-4334-b82f-4098f8101b8e'),\n",
       "  HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='978c5962-83c6-4b7a-979b-580754cfca76'),\n",
       "  HumanMessage(content='You just asked me what we were discussing before our current conversation. Would you like me to recall that conversation or start fresh with a new topic?', additional_kwargs={}, response_metadata={}, id='747a6517-1db6-4c42-88d3-f3963572a916')]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"3\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1745b-8d53-4ce7-8291-53d6f524721c",
   "metadata": {},
   "source": [
    "## Multimodal with image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5850537-3c6b-4751-929f-ac601f4e5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import httpx\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from langchain_ollama import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c78fe507-33bc-4cd0-b8a6-ac124cde2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
    "with open(\"/Users/edwinhuang/Desktop/IMG_2198.JPG\", \"rb\") as image_file:\n",
    "    base64_encoded_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "image_file.close()\n",
    "\n",
    "pil_image = Image.open(\"/Users/edwinhuang/Desktop/IMG_2198.JPG\")\n",
    "# image_64 = convert_to_base64(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da511c5-138d-45e7-abaf-c5ecf3c77f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf4e66-adf1-4841-a355-b34fd6d4da0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "28837752-bf62-4be7-8d03-5277fd1e1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"describe the image\"}],\n",
    "    images = [{'type' : 'image', \n",
    "               'image': \"/Users/edwinhuang/Desktop/IMG_2198.JPG\"}]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5499a40a-4a72-43b4-955a-2da36e1ca8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have the ability to access or view images on your desktop. I'm a text-based AI assistant and do not have direct access to external files or URLs.\\n\\nHowever, if you'd like to describe the image yourself, I can help facilitate that! What's the image of? Is it a person, place, object, or something else entirely?\", additional_kwargs={}, response_metadata={'model': 'llama3.2-vision', 'created_at': '2024-11-15T03:47:30.271978Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 11238036584, 'load_duration': 6225657709, 'prompt_eval_count': 28, 'prompt_eval_duration': 1677000000, 'eval_count': 73, 'eval_duration': 3240000000}, id='run-bd36598f-e1d8-40ec-a756-047b9bc51e3a-0', usage_metadata={'input_tokens': 28, 'output_tokens': 73, 'total_tokens': 101})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(f\"Describe this image: /Users/edwinhuang/Desktop/IMG_2198.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d5884f-77af-4a01-9835-0c79f677f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dbcb73a-7431-4222-b057-cee52a3f7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model = \"llama3.2-vision\",\n",
    "    temperature = 0,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a4631a7-bbb8-4758-b60b-e58f6a5e7ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The image shows a man standing in front of a crowd of people, with a building and palm trees visible in the background. The purpose of the image is to capture a moment or event, possibly a conference or gathering.\\n\\nHere are the details of the image:\\n\\n* A man:\\n\\t+ Standing in the foreground\\n\\t+ Wearing a blue vest over a brown shirt\\n\\t+ Holding a white card with black text\\n\\t+ Looking at the camera\\n* A crowd of people:\\n\\t+ Seated behind the man\\n\\t+ Facing away from the camera\\n\\t+ Some are standing up, while others are sitting down\\n\\t+ They appear to be engaged in conversation or listening to someone speak\\n* A building:\\n\\t+ Visible in the background\\n\\t+ Has a large window and a door\\n\\t+ Appears to be a conference center or meeting room\\n* Palm trees:\\n\\t+ Visible in the distance\\n\\t+ Tall and slender, with long leaves\\n\\nOverall, the image suggests that the man is attending an event or conference, possibly related to business or technology. The crowd of people behind him implies that there are many others present, and the building in the background provides a sense of context for the gathering.' additional_kwargs={} response_metadata={'model': 'llama3.2-vision', 'created_at': '2024-11-13T20:57:34.909865Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 110548706542, 'load_duration': 101591833, 'prompt_eval_count': 16, 'prompt_eval_duration': 91572000000, 'eval_count': 248, 'eval_duration': 18298000000} id='run-b126dde9-b9a0-4ec0-a3c4-465c6c6ee90b-0' usage_metadata={'input_tokens': 16, 'output_tokens': 248, 'total_tokens': 264}\n"
     ]
    }
   ],
   "source": [
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"describe the image\"},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_encoded_data}\"},\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(llm.invoke([message]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f815cf-4cc7-4e80-b183-cb8083e1743a",
   "metadata": {},
   "source": [
    "## Integrate using AWS Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "61c76afa-28ef-4040-80a4-9ca517ca57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock, BedrockLLM, ChatBedrockConverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "610c7ddf-4d63-4772-8374-3ae248047647",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrockConverse(\n",
    "    model_id='us.meta.llama3-2-11b-instruct-v1:0',\n",
    "    temperature = 0,\n",
    "    max_tokens=None,\n",
    "    \n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "35dc4d66-d93c-4c3f-bbd1-55dbf7172d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatBedrockConverse(disable_streaming='tool_calling', client=<botocore.client.BedrockRuntime object at 0x31a080710>, model_id='us.meta.llama3-2-11b-instruct-v1:0', temperature=0.0, region_name='us-east-1', provider='meta', supports_tool_choice_values=())"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a4e143b8-6000-4515-aec2-5ff9fcd00194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m an AI, a large language model, and I don\\'t have a physical presence or personal experiences. I exist solely in the digital realm, designed to assist and communicate with users like you.\\n\\nHowever, I can try to create a fictional scenario for you. Let me imagine a character who might be running from something.\\n\\nMeet \"Echo,\" a mysterious figure with a troubled past. Echo is a skilled hacker and digital nomad, always on the move, never staying in one place for too long. They\\'re running from a powerful corporation that\\'s been tracking their every move, trying to exploit their exceptional skills for their own gain.\\n\\nEcho\\'s past is shrouded in secrecy, but it\\'s rumored that they were once a key player in a high-stakes cyber-heist that went horribly wrong. Now, they\\'re on the run, using their expertise to stay one step ahead of their pursuers.\\n\\nAs Echo navigates the darknet and encrypted channels, they\\'re constantly looking over their shoulder, waiting for the other shoe to drop. But they won\\'t be caught without a fight. Echo is a master of disguise, able to blend into the shadows and disappear at will.\\n\\nSo, that\\'s Echo\\'s story. But I\\'m curious, who are you, and what are you running from?', additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'e5bd6951-22ba-4d58-8f56-5633c5899b9d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 14 Nov 2024 20:28:38 GMT', 'content-type': 'application/json', 'content-length': '1399', 'connection': 'keep-alive', 'x-amzn-requestid': 'e5bd6951-22ba-4d58-8f56-5633c5899b9d'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [2193]}}, id='run-ecbb3868-59dd-4628-89dd-098fa0409a30-0', usage_metadata={'input_tokens': 44, 'output_tokens': 262, 'total_tokens': 306})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = llm.invoke('who are you, where are you running from')\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a7b512b5-a444-4afb-9d04-3881e8be38db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used: {'input_tokens': 44, 'output_tokens': 262, 'total_tokens': 306}\n"
     ]
    }
   ],
   "source": [
    "usage = resp.dict()['usage_metadata']\n",
    "print(f'Used: {usage}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d5f7cda6-d952-4bef-bbc4-3aa849e3f631",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationException",
     "evalue": "An error occurred (ValidationException) when calling the Converse operation: The model returned the following errors: Image exceeds max pixels allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m image_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      5\u001b[0m message \u001b[38;5;241m=\u001b[39m HumanMessage(\n\u001b[1;32m      6\u001b[0m     content\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      7\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescribe the weather in this image\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     ],\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_aws/chat_models/bedrock_converse.py:495\u001b[0m, in \u001b[0;36mChatBedrockConverse._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m bedrock_messages, system \u001b[38;5;241m=\u001b[39m _messages_to_bedrock(messages)\n\u001b[1;32m    492\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_converse_params(\n\u001b[1;32m    493\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_snake_to_camel_keys(kwargs, excluded_keys\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m    494\u001b[0m )\n\u001b[0;32m--> 495\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbedrock_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m response_message \u001b[38;5;241m=\u001b[39m _parse_response(response)\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[ChatGeneration(message\u001b[38;5;241m=\u001b[39mresponse_message)])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the Converse operation: The model returned the following errors: Image exceeds max pixels allowed."
     ]
    }
   ],
   "source": [
    "image_filename='/Users/edwinhuang/Desktop/Screenshot 2024-10-29 at 2.09.16 PM.png'\n",
    "with open(image_filename, \"rb\") as image_file:\n",
    "    base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "image_file.close()\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\"type\": \"base64\", \"media_type\": \"image/jpeg\", \"data\": base64_encoded_data},\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "resp = llm.invoke([message])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5adc32c6-4657-43cd-93bf-32f20f5a59a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m an AI, a large language model, and I don\\'t have a physical presence or personal experiences. I exist solely in the digital realm, designed to assist and communicate with users like you.\\n\\nHowever, I can try to create a fictional scenario for you. Let me imagine a character who might be running from something.\\n\\nMeet \"Echo,\" a mysterious figure with a troubled past. Echo is a skilled hacker and digital nomad, always on the move, never staying in one place for too long. They\\'re running from a powerful corporation that\\'s been tracking their every move, trying to exploit their exceptional skills for their own gain.\\n\\nEcho\\'s past is shrouded in secrecy, but it\\'s rumored that they were once a key player in a high-stakes cyber-heist that went horribly wrong. Now, they\\'re on the run, using their expertise to stay one step ahead of their pursuers.\\n\\nAs Echo navigates the darknet and encrypted channels, they\\'re constantly looking over their shoulder, waiting for the other shoe to drop. But they won\\'t be caught without a fight. Echo is a master of disguise, able to blend into the shadows and disappear at will.\\n\\nSo, that\\'s Echo\\'s story. But I\\'m curious, who are you, and what are you running from?', additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'e5bd6951-22ba-4d58-8f56-5633c5899b9d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 14 Nov 2024 20:28:38 GMT', 'content-type': 'application/json', 'content-length': '1399', 'connection': 'keep-alive', 'x-amzn-requestid': 'e5bd6951-22ba-4d58-8f56-5633c5899b9d'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [2193]}}, id='run-ecbb3868-59dd-4628-89dd-098fa0409a30-0', usage_metadata={'input_tokens': 44, 'output_tokens': 262, 'total_tokens': 306})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "76d9e9fe-2a1b-4c2f-a1c4-df8324c909cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import httpx\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Define the maximum allowed dimensions\n",
    "max_width = 1024  # Replace with AWS Bedrock's maximum width\n",
    "max_height = 1024  # Replace with AWS Bedrock's maximum height\n",
    "\n",
    "# Fetch the image\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "response = httpx.get(image_url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Resize the image if it exceeds the maximum dimensions\n",
    "if image.width > max_width or image.height > max_height:\n",
    "    image.thumbnail((max_width, max_height))\n",
    "\n",
    "# Convert the image to JPEG format\n",
    "buffer = BytesIO()\n",
    "image.save(buffer, format=\"JPEG\")\n",
    "buffer.seek(0)\n",
    "\n",
    "# Encode the image to base64\n",
    "image_data = base64.b64encode(buffer.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\"type\": \"base64\", \"media_type\": \"image/jpeg\", \"data\": image_data},\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "resp = llm.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7142e3d9-756a-4313-a329-fe5b03a38386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The weather in this image is characterized by a blue sky with scattered clouds, indicating a partly cloudy day. The sun is shining, casting a warm glow over the scene. The temperature appears to be mild, with no visible signs of extreme heat or cold. The atmosphere is serene and peaceful, with a gentle breeze rustling the grasses and plants. Overall, the weather in this image suggests a pleasant and comfortable day, ideal for outdoor activities such as walking or hiking on the boardwalk.', additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'f57c8e9a-0599-4acd-b98f-b34ec2851cc1', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 14 Nov 2024 21:31:03 GMT', 'content-type': 'application/json', 'content-length': '677', 'connection': 'keep-alive', 'x-amzn-requestid': 'f57c8e9a-0599-4acd-b98f-b34ec2851cc1'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [1604]}}, id='run-b2a3d489-3d33-43ab-99f0-433686e41804-0', usage_metadata={'input_tokens': 954, 'output_tokens': 99, 'total_tokens': 1053})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f807e-2a70-4c19-97d5-5608a3182c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
